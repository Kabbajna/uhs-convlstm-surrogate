# -*- coding: utf-8 -*-
"""FNO.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/110vSAsAJPDHpi2vd1u9hn0jnjj918G5p
"""

"""
FNO (Fourier Neural Operator) 3D pour UHS
Ã‰tat de l'art pour rÃ©solution de PDEs (Li et al., 2021)

SPLIT 70/15/15 - SEED=42 (mÃªme que ConvLSTM et 3D U-Net pour comparaison Ã©quitable)
AVEC REPRISE AUTOMATIQUE DEPUIS CHECKPOINT

RÃ©fÃ©rence: "Fourier Neural Operator for Parametric Partial Differential Equations"
https://arxiv.org/abs/2010.08895
"""

# ============================================================================
# SETUP
# ============================================================================

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

from pathlib import Path
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from torch.utils.data import Dataset, DataLoader
import scipy.io as sio
import matplotlib.pyplot as plt
import gc
from tqdm import tqdm
import pickle
import random
import json
from datetime import datetime

# ============================================================================
# FIXER TOUS LES SEEDS POUR REPRODUCTIBILITÃ‰
# ============================================================================
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

print(f"ðŸŽ² SEED = {SEED} (rÃ©sultats reproductibles)")

DATA_PATH = Path('/content/drive/MyDrive/Deeponet/multifidelity_deeponet_data')
CHECKPOINT_DIR = DATA_PATH / 'checkpoints' / 'fno_3d'
CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

print(f"Device: {device}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
print(f"Data path exists: {DATA_PATH.exists()}")

# ============================================================================
# SPECTRAL CONVOLUTION 3D (Coeur du FNO)
# ============================================================================

class SpectralConv3d(nn.Module):
    """
    3D Fourier layer - apprentissage dans l'espace des frÃ©quences

    Effectue:
    1. FFT 3D
    2. Multiplication par poids apprenables (modes basses frÃ©quences)
    3. IFFT 3D
    """
    def __init__(self, in_channels, out_channels, modes1, modes2, modes3):
        super().__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.modes1 = modes1  # Nombre de modes Fourier en x
        self.modes2 = modes2  # Nombre de modes Fourier en y
        self.modes3 = modes3  # Nombre de modes Fourier en z

        self.scale = (1 / (in_channels * out_channels))

        # Poids complexes pour les 4 "coins" du spectre 3D
        self.weights1 = nn.Parameter(
            self.scale * torch.rand(in_channels, out_channels, modes1, modes2, modes3, dtype=torch.cfloat))
        self.weights2 = nn.Parameter(
            self.scale * torch.rand(in_channels, out_channels, modes1, modes2, modes3, dtype=torch.cfloat))
        self.weights3 = nn.Parameter(
            self.scale * torch.rand(in_channels, out_channels, modes1, modes2, modes3, dtype=torch.cfloat))
        self.weights4 = nn.Parameter(
            self.scale * torch.rand(in_channels, out_channels, modes1, modes2, modes3, dtype=torch.cfloat))

    def compl_mul3d(self, input, weights):
        """Multiplication complexe pour tenseurs 3D"""
        # (batch, in_channel, x, y, z), (in_channel, out_channel, x, y, z) -> (batch, out_channel, x, y, z)
        return torch.einsum("bixyz,ioxyz->boxyz", input, weights)

    def forward(self, x):
        batchsize = x.shape[0]

        # FFT 3D
        x_ft = torch.fft.rfftn(x, dim=[-3, -2, -1])

        # Multiplier les modes pertinents
        out_ft = torch.zeros(batchsize, self.out_channels, x.size(-3), x.size(-2), x.size(-1)//2 + 1,
                            dtype=torch.cfloat, device=x.device)

        # Les 4 "coins" du spectre 3D (combinaisons de basses/hautes frÃ©quences)
        out_ft[:, :, :self.modes1, :self.modes2, :self.modes3] = \
            self.compl_mul3d(x_ft[:, :, :self.modes1, :self.modes2, :self.modes3], self.weights1)
        out_ft[:, :, -self.modes1:, :self.modes2, :self.modes3] = \
            self.compl_mul3d(x_ft[:, :, -self.modes1:, :self.modes2, :self.modes3], self.weights2)
        out_ft[:, :, :self.modes1, -self.modes2:, :self.modes3] = \
            self.compl_mul3d(x_ft[:, :, :self.modes1, -self.modes2:, :self.modes3], self.weights3)
        out_ft[:, :, -self.modes1:, -self.modes2:, :self.modes3] = \
            self.compl_mul3d(x_ft[:, :, -self.modes1:, -self.modes2:, :self.modes3], self.weights4)

        # IFFT 3D
        x = torch.fft.irfftn(out_ft, s=(x.size(-3), x.size(-2), x.size(-1)))
        return x


# ============================================================================
# FNO 3D
# ============================================================================

class FNO3d(nn.Module):
    """
    Fourier Neural Operator 3D

    Architecture:
    1. Lifting: Projection de l'input vers espace latent (width canaux)
    2. Fourier Layers: N couches spectrales + convolutions locales
    3. Projection: Retour vers espace de sortie

    Chaque Fourier Layer = SpectralConv3d + Conv3d(1x1x1) + Activation
    """
    def __init__(self, in_channels=5, out_channels=3, width=32, modes1=8, modes2=8, modes3=4, num_layers=4):
        super().__init__()

        self.in_channels = in_channels
        self.out_channels = out_channels
        self.width = width
        self.modes1 = modes1
        self.modes2 = modes2
        self.modes3 = modes3
        self.num_layers = num_layers

        # Lifting layer
        self.fc0 = nn.Conv3d(in_channels, width, kernel_size=1)

        # Fourier layers
        self.spectral_layers = nn.ModuleList()
        self.conv_layers = nn.ModuleList()
        self.norms = nn.ModuleList()

        for _ in range(num_layers):
            self.spectral_layers.append(SpectralConv3d(width, width, modes1, modes2, modes3))
            self.conv_layers.append(nn.Conv3d(width, width, kernel_size=1))
            self.norms.append(nn.BatchNorm3d(width))

        # Projection layers
        self.fc1 = nn.Conv3d(width, 128, kernel_size=1)
        self.fc2 = nn.Conv3d(128, out_channels, kernel_size=1)

        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        """
        Forward pass
        Args:
            x: [batch, in_channels, nx, ny, nz]
        Returns:
            out: [batch, out_channels, nx, ny, nz]
        """
        # Lifting
        x = self.fc0(x)

        # Fourier layers
        for i in range(self.num_layers):
            x1 = self.spectral_layers[i](x)
            x2 = self.conv_layers[i](x)
            x = x1 + x2
            x = self.norms[i](x)
            if i < self.num_layers - 1:
                x = F.gelu(x)

        # Projection
        x = self.fc1(x)
        x = F.gelu(x)
        x = self.fc2(x)
        x = self.sigmoid(x)

        return x


# ============================================================================
# DATASET (MÃŠME QUE LES AUTRES MODÃˆLES)
# ============================================================================

class SequenceUHSDataset(Dataset):
    def __init__(self, data_path, fidelity='medium', sequence_length=10):
        self.data_dir = data_path / f"{fidelity}_fidelity"
        self.sequence_length = sequence_length
        self.fidelity = fidelity

        self.mat_files = sorted(list(self.data_dir.glob("*.mat")))
        self.sequences = self._create_sequences()

        print(f"Dataset {fidelity}: {len(self.mat_files)} sims, {len(self.sequences)} sequences")

    def _load_mat_flexible(self, mat_path):
        try:
            data = sio.loadmat(mat_path, struct_as_record=False, squeeze_me=True)

            if 'sim_data' not in data:
                return None

            sim = data['sim_data']
            if isinstance(sim, np.ndarray) and sim.ndim == 2:
                sim = sim[0, 0]

            if not hasattr(sim, 'timesteps'):
                return None

            timesteps_raw = sim.timesteps.flatten() if hasattr(sim.timesteps, 'flatten') else sim.timesteps
            if not isinstance(timesteps_raw, np.ndarray):
                timesteps_raw = [timesteps_raw]

            if hasattr(sim, 'params'):
                params = sim.params
                if isinstance(params, np.ndarray) and params.ndim == 2:
                    params = params[0, 0]
                K = float(params.K[0, 0]) if hasattr(params.K, 'shape') else float(params.K)
                phi = float(params.phi[0, 0]) if hasattr(params.phi, 'shape') else float(params.phi)
            else:
                K = 1e-13
                phi = 0.2

            timesteps = []
            for ts in timesteps_raw:
                def safe_extract(field, default=0):
                    val = getattr(ts, field, default)
                    if hasattr(val, 'shape') and val.ndim >= 1:
                        return val[0, 0] if val.ndim == 2 else val[0]
                    return val

                timestep_obj = type('obj', (object,), {
                    'Sw': np.array(ts.Sw, dtype=np.float32),
                    'Sg': np.array(ts.Sg, dtype=np.float32),
                    'P': np.array(ts.P, dtype=np.float32),
                    'Sg_max': np.array(ts.Sg_max, dtype=np.float32),
                    'time': float(safe_extract('time', 0)),
                    'Q': float(safe_extract('Q', 0)),
                    'operation': str(safe_extract('operation', 'injection'))
                })()
                timesteps.append(timestep_obj)

            return {'timesteps': timesteps, 'K': K, 'phi': phi}

        except Exception as e:
            return None

    def _create_sequences(self):
        sequences = []
        error_count = 0

        for mat_file in self.mat_files:
            result = self._load_mat_flexible(mat_file)

            if result is None:
                error_count += 1
                continue

            timesteps = result['timesteps']

            if len(timesteps) < self.sequence_length + 1:
                continue

            for i in range(len(timesteps) - self.sequence_length):
                sequences.append({
                    'file': mat_file,
                    'start_idx': i,
                    'K': result['K'],
                    'phi': result['phi']
                })

        if error_count > 0:
            print(f"Skipped {error_count} files")

        return sequences

    def __len__(self):
        return len(self.sequences)

    def __getitem__(self, idx):
        seq_info = self.sequences[idx]

        result = self._load_mat_flexible(seq_info['file'])
        if result is None:
            raise ValueError(f"Cannot load {seq_info['file']}")

        timesteps = result['timesteps']
        start = seq_info['start_idx']
        sequence = timesteps[start:start + self.sequence_length + 1]

        inputs = []
        targets = []

        for i in range(self.sequence_length):
            current = sequence[i]
            next_step = sequence[i + 1]

            Sw = np.array(current.Sw, dtype=np.float32)
            Sg = np.array(current.Sg, dtype=np.float32)
            P = np.array(current.P, dtype=np.float32) / 14.8e6
            Sg_max = np.array(current.Sg_max, dtype=np.float32)
            meta = np.ones_like(Sw) * (seq_info['K'] / 1e-13)

            input_state = np.stack([Sw, Sg, P, Sg_max, meta], axis=0)

            target_state = np.stack([
                np.array(next_step.Sg, dtype=np.float32),
                np.array(next_step.Sw, dtype=np.float32),
                np.array(next_step.P, dtype=np.float32) / 14.8e6
            ], axis=0)

            inputs.append(input_state)
            targets.append(target_state)

        return {
            'inputs': np.array(inputs),
            'targets': np.array(targets),
            'K': seq_info['K'],
            'phi': seq_info['phi']
        }


# ============================================================================
# LOSS PHYSICS-INFORMED (MÃŠME QUE LES AUTRES MODÃˆLES)
# ============================================================================

class PhysicsInformedLoss(nn.Module):
    def __init__(self, lambda_data=1.0, lambda_sat=0.5, lambda_mass=0.2, lambda_darcy=0.1):
        super().__init__()
        self.lambda_data = lambda_data
        self.lambda_sat = lambda_sat
        self.lambda_mass = lambda_mass
        self.lambda_darcy = lambda_darcy

    def forward(self, pred, target, input_state=None, metadata=None):
        loss_data = F.mse_loss(pred, target)

        Sg_pred = pred[:, 0]
        Sw_pred = pred[:, 1]
        loss_sat = torch.mean((Sg_pred + Sw_pred - 1.0) ** 2)

        loss_bounds = torch.mean(F.relu(-Sg_pred)) + torch.mean(F.relu(Sg_pred - 1.0))
        loss_bounds += torch.mean(F.relu(-Sw_pred)) + torch.mean(F.relu(Sw_pred - 1.0))

        loss_mass = torch.tensor(0.0, device=pred.device)

        if input_state is not None and metadata is not None:
            phi = metadata.get('phi', 0.2)
            dt = metadata.get('dt', 1.0)

            Sg_t = input_state[:, 1]
            dSg_dt = (Sg_pred - Sg_t) / (dt + 1e-8)

            operation = metadata.get('operation', 0.5)

            if operation > 0.5:
                loss_mass_dir = torch.mean(F.relu(-dSg_dt))
            else:
                loss_mass_dir = torch.mean(F.relu(dSg_dt))

            loss_mass_rate = torch.mean(torch.abs(phi * dSg_dt) ** 2) * 0.01
            loss_mass = loss_mass_dir + loss_mass_rate

        loss_darcy = torch.tensor(0.0, device=pred.device)

        if input_state is not None and metadata is not None:
            Sg_grad_x = Sg_pred[:, 1:, :, :] - Sg_pred[:, :-1, :, :]
            Sg_grad_y = Sg_pred[:, :, 1:, :] - Sg_pred[:, :, :-1, :]
            Sg_grad_z = Sg_pred[:, :, :, 1:] - Sg_pred[:, :, :, :-1]

            max_grad = 0.3
            loss_darcy_x = torch.mean(F.relu(torch.abs(Sg_grad_x) - max_grad))
            loss_darcy_y = torch.mean(F.relu(torch.abs(Sg_grad_y) - max_grad))
            loss_darcy_z = torch.mean(F.relu(torch.abs(Sg_grad_z) - max_grad))

            mask_gas = (Sg_pred[:, :, :, :-1] > 0.01).float()
            loss_gravity = -torch.mean(Sg_grad_z * mask_gas)
            loss_gravity = torch.clamp(loss_gravity, min=0)

            loss_darcy = loss_darcy_x + loss_darcy_y + loss_darcy_z + loss_gravity * 0.1

        total_loss = (self.lambda_data * loss_data +
                     self.lambda_sat * (loss_sat + loss_bounds) +
                     self.lambda_mass * loss_mass +
                     self.lambda_darcy * loss_darcy)

        return total_loss, {
            'data': loss_data.item(),
            'sat': loss_sat.item(),
            'bounds': loss_bounds.item(),
            'mass': loss_mass.item() if isinstance(loss_mass, torch.Tensor) else 0.0,
            'darcy': loss_darcy.item() if isinstance(loss_darcy, torch.Tensor) else 0.0
        }


# ============================================================================
# TRAINING FNO (PAS DE MÃ‰MOIRE ENTRE TIMESTEPS - COMME 3D U-NET)
# ============================================================================

def train_epoch_fno(model, dataloader, optimizer, criterion, device, epoch):
    """
    Training pour FNO
    Chaque timestep est traitÃ© INDÃ‰PENDAMMENT (pas de mÃ©moire temporelle)
    """
    model.train()
    total_loss = 0
    loss_components = {'data': 0, 'sat': 0, 'bounds': 0, 'mass': 0, 'darcy': 0}
    all_preds = []
    all_targets = []

    pbar = tqdm(dataloader, desc=f'Epoch {epoch}')

    for batch_idx, batch in enumerate(pbar):
        inputs = batch['inputs'].to(device)
        targets = batch['targets'].to(device)

        batch_size, seq_len, in_channels, nx, ny, nz = inputs.shape

        optimizer.zero_grad()

        total_batch_loss = 0

        # Traiter chaque timestep INDÃ‰PENDAMMENT
        for t in range(seq_len):
            pred = model(inputs[:, t])

            metadata = {
                'phi': batch['phi'].mean().item(),
                'K': batch['K'].mean().item(),
                'dt': 1.0,
                'operation': 1.0
            }

            loss, components = criterion(pred, targets[:, t], input_state=inputs[:, t], metadata=metadata)

            total_batch_loss += loss

            for key in loss_components:
                loss_components[key] += components[key]

            all_preds.append(pred[:, 0].detach().cpu().numpy().flatten())
            all_targets.append(targets[:, t, 0].cpu().numpy().flatten())

        total_batch_loss = total_batch_loss / seq_len
        total_batch_loss.backward()

        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()

        total_loss += total_batch_loss.item()

        pbar.set_postfix({'loss': f'{total_batch_loss.item():.6f}'})

    all_preds = np.concatenate(all_preds)
    all_targets = np.concatenate(all_targets)
    ss_res = np.sum((all_targets - all_preds) ** 2)
    ss_tot = np.sum((all_targets - np.mean(all_targets)) ** 2)
    r2 = 1 - (ss_res / ss_tot)

    n = len(dataloader) * seq_len
    return total_loss / len(dataloader), {k: v/n for k, v in loss_components.items()}, r2


def validate_fno(model, dataloader, criterion, device):
    """Validation single-step pour FNO"""
    model.eval()
    total_loss = 0
    all_preds = []
    all_targets = []

    with torch.no_grad():
        for batch in dataloader:
            inputs = batch['inputs'].to(device)
            targets = batch['targets'].to(device)

            batch_size, seq_len, in_channels, nx, ny, nz = inputs.shape

            batch_loss = 0

            for t in range(seq_len):
                pred = model(inputs[:, t])
                loss, _ = criterion(pred, targets[:, t])
                batch_loss += loss

                all_preds.append(pred[:, 0].detach().cpu().numpy().flatten())
                all_targets.append(targets[:, t, 0].cpu().numpy().flatten())

            total_loss += (batch_loss / seq_len).item()

    all_preds = np.concatenate(all_preds)
    all_targets = np.concatenate(all_targets)
    ss_res = np.sum((all_targets - all_preds) ** 2)
    ss_tot = np.sum((all_targets - np.mean(all_targets)) ** 2)
    r2 = 1 - (ss_res / ss_tot)

    return total_loss / len(dataloader), r2


def evaluate_rollout_fno(model, dataloader, device, rollout_steps=10):
    """
    Ã‰valuation rollout pour FNO
    Pas de mÃ©moire entre les prÃ©dictions - utilise juste sa prÃ©diction prÃ©cÃ©dente
    """
    model.eval()
    all_preds = []
    all_targets = []

    with torch.no_grad():
        for batch in tqdm(dataloader, desc=f'Rollout {rollout_steps}-step'):
            inputs = batch['inputs'].to(device)
            targets = batch['targets'].to(device)

            batch_size, seq_len, in_channels, nx, ny, nz = inputs.shape

            current_input = inputs[:, 0]

            for t in range(min(rollout_steps, seq_len)):
                pred = model(current_input)

                all_preds.append(pred[:, 0].cpu().numpy().flatten())
                all_targets.append(targets[:, t, 0].cpu().numpy().flatten())

                if t < seq_len - 1:
                    Sg_pred = pred[:, 0:1]
                    Sw_pred = pred[:, 1:2]
                    P_pred = pred[:, 2:3]

                    Sg_max_prev = inputs[:, t, 3:4]
                    Sg_max_new = torch.maximum(Sg_max_prev, Sg_pred)
                    meta = inputs[:, t, 4:5]

                    current_input = torch.cat([Sw_pred, Sg_pred, P_pred, Sg_max_new, meta], dim=1)

    all_preds = np.concatenate(all_preds)
    all_targets = np.concatenate(all_targets)
    ss_res = np.sum((all_targets - all_preds) ** 2)
    ss_tot = np.sum((all_targets - np.mean(all_targets)) ** 2)
    r2 = 1 - (ss_res / ss_tot)

    return r2


# ============================================================================
# MAIN
# ============================================================================

print(f"\n{'='*70}")
print("FNO (FOURIER NEURAL OPERATOR) 3D")
print("Ã‰tat de l'art pour PDEs (Li et al., 2021)")
print("SPLIT 70/15/15 - SEED=42")
print("AVEC REPRISE AUTOMATIQUE")
print(f"{'='*70}\n")

# Config
fidelity = 'medium'
sequence_length = 10
batch_size = 1
num_epochs = 40
learning_rate = 1e-3  # FNO fonctionne mieux avec LR plus Ã©levÃ©

# FNO hyperparameters
width = 32          # Largeur du rÃ©seau (canaux latents)
modes1 = 8          # Modes Fourier en x
modes2 = 8          # Modes Fourier en y
modes3 = 4          # Modes Fourier en z
num_layers = 4      # Nombre de Fourier layers

print(f"Configuration:")
print(f"  Model: FNO 3D (Fourier Neural Operator)")
print(f"  Fidelity: {fidelity}")
print(f"  Sequence length: {sequence_length}")
print(f"  Batch size: {batch_size}")
print(f"  Epochs: {num_epochs}")
print(f"  Learning rate: {learning_rate}")
print(f"  Width: {width}")
print(f"  Fourier modes: ({modes1}, {modes2}, {modes3})")
print(f"  Num layers: {num_layers}")
print(f"  Split: 70/15/15")
print(f"  Seed: {SEED}\n")

# Dataset
dataset = SequenceUHSDataset(DATA_PATH, fidelity, sequence_length)

if len(dataset) == 0:
    raise ValueError("No sequences created!")

# SPLIT 70/15/15 AVEC SEED (IDENTIQUE AUX AUTRES MODÃˆLES)
train_size = int(0.70 * len(dataset))
val_size = int(0.15 * len(dataset))
test_size = len(dataset) - train_size - val_size

generator = torch.Generator().manual_seed(SEED)
train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(
    dataset, [train_size, val_size, test_size], generator=generator
)

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,
                          num_workers=0, pin_memory=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,
                        num_workers=0, pin_memory=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,
                         num_workers=0, pin_memory=True)

print(f"Split 70/15/15 (seed={SEED}):")
print(f"  Train: {len(train_dataset)}")
print(f"  Val:   {len(val_dataset)}")
print(f"  Test:  {len(test_dataset)}\n")

# Model
model = FNO3d(
    in_channels=5,
    out_channels=3,
    width=width,
    modes1=modes1,
    modes2=modes2,
    modes3=modes3,
    num_layers=num_layers
).to(device)

n_params = sum(p.numel() for p in model.parameters())
print(f"Model: FNO 3D")
print(f"Parameters: {n_params:,}")
print(f"Note: Apprentissage dans l'espace de Fourier - efficace pour PDEs\n")

# Loss & Optimizer
criterion = PhysicsInformedLoss(lambda_data=1.0, lambda_sat=0.5, lambda_mass=0.2, lambda_darcy=0.1)
optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)

# Training
best_val_loss = float('inf')
best_val_r2 = -float('inf')
history = {'train_loss': [], 'val_loss': [], 'train_r2': [], 'val_r2': [], 'lr': []}

# ============================================================================
# REPRISE AUTOMATIQUE DEPUIS CHECKPOINT
# ============================================================================

checkpoint_file = CHECKPOINT_DIR / 'latest_fno_3d.pt'
start_epoch = 1

if checkpoint_file.exists():
    print(f"\n{'='*70}")
    print("ðŸ”„ REPRISE DEPUIS CHECKPOINT")
    print(f"{'='*70}")
    try:
        checkpoint = torch.load(checkpoint_file, map_location=device, weights_only=False)
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        start_epoch = checkpoint['epoch'] + 1
        best_val_loss = checkpoint.get('best_val_loss', float('inf'))
        best_val_r2 = checkpoint.get('best_val_r2', -float('inf'))

        # Charger historique
        history_file = CHECKPOINT_DIR / 'history_fno_3d.pkl'
        if history_file.exists():
            with open(history_file, 'rb') as f:
                history = pickle.load(f)

        # Reconfigurer scheduler pour reprendre au bon epoch
        for _ in range(start_epoch - 1):
            scheduler.step()

        print(f"âœ“ Reprise Ã  l'epoch {start_epoch}")
        print(f"âœ“ Meilleur val RÂ²: {best_val_r2:.4f}")
        print(f"âœ“ Historique: {len(history['train_loss'])} epochs chargÃ©s\n")
    except Exception as e:
        print(f"âš ï¸ Erreur chargement checkpoint: {e}")
        print("RedÃ©marrage depuis epoch 1\n")
        start_epoch = 1
else:
    print("ðŸ“ Aucun checkpoint trouvÃ©, dÃ©marrage depuis epoch 1\n")

# Sauvegarder config
config = {
    'model': 'FNO_3D',
    'seed': SEED,
    'split': '70/15/15',
    'fidelity': fidelity,
    'sequence_length': sequence_length,
    'batch_size': batch_size,
    'num_epochs': num_epochs,
    'learning_rate': learning_rate,
    'width': width,
    'modes': [modes1, modes2, modes3],
    'num_layers': num_layers,
    'n_params': n_params,
    'train_size': len(train_dataset),
    'val_size': len(val_dataset),
    'test_size': len(test_dataset),
    'timestamp': datetime.now().isoformat()
}

with open(CHECKPOINT_DIR / 'config.json', 'w') as f:
    json.dump(config, f, indent=2)

# ============================================================================
# TRAINING LOOP (avec reprise)
# ============================================================================

for epoch in range(start_epoch, num_epochs + 1):
    print(f"\n{'='*70}")
    print(f"Epoch {epoch}/{num_epochs} - FNO 3D")
    print(f"{'='*70}")

    train_loss, components, train_r2 = train_epoch_fno(model, train_loader, optimizer, criterion, device, epoch)
    val_loss, val_r2 = validate_fno(model, val_loader, criterion, device)

    current_lr = optimizer.param_groups[0]['lr']

    history['train_loss'].append(train_loss)
    history['val_loss'].append(val_loss)
    history['train_r2'].append(train_r2)
    history['val_r2'].append(val_r2)
    history['lr'].append(current_lr)

    print(f"\nTrain: Loss={train_loss:.6f}, RÂ²={train_r2:.4f}")
    print(f"  - Data: {components['data']:.6f}")
    print(f"  - Saturation: {components['sat']:.6f}")
    print(f"  - Mass: {components['mass']:.6f}")
    print(f"  - Darcy: {components['darcy']:.6f}")
    print(f"Val: Loss={val_loss:.6f}, RÂ²={val_r2:.4f}")
    print(f"LR: {current_lr:.2e}")

    scheduler.step()

    # Sauvegarder meilleur modÃ¨le
    if val_r2 > best_val_r2:
        best_val_r2 = val_r2
        best_val_loss = val_loss
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'val_loss': val_loss,
            'val_r2': val_r2,
            'best_val_loss': best_val_loss,
            'best_val_r2': best_val_r2,
            'history': history
        }, CHECKPOINT_DIR / "best_fno_3d.pt")
        print(f"  âœ“ Best model saved (RÂ²={val_r2:.4f})")

    # Checkpoint latest (TOUJOURS sauvegarder pour reprise)
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'val_loss': val_loss,
        'val_r2': val_r2,
        'best_val_loss': best_val_loss,
        'best_val_r2': best_val_r2,
        'history': history
    }, checkpoint_file)

    # Sauvegarder historique
    with open(CHECKPOINT_DIR / 'history_fno_3d.pkl', 'wb') as f:
        pickle.dump(history, f)

    if epoch % 5 == 0:
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))

        # Loss
        axes[0, 0].plot(history['train_loss'], 'b-', label='Train', linewidth=2)
        axes[0, 0].plot(history['val_loss'], 'r-', label='Val', linewidth=2)
        axes[0, 0].set_yscale('log')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].legend()
        axes[0, 0].grid(alpha=0.3)
        axes[0, 0].set_title('Loss - FNO 3D')

        # RÂ²
        axes[0, 1].plot(history['train_r2'], 'b-', label='Train', linewidth=2)
        axes[0, 1].plot(history['val_r2'], 'r-', label='Val', linewidth=2)
        axes[0, 1].axhline(y=0.99, color='green', linestyle='--', alpha=0.5, label='Target')
        axes[0, 1].set_ylim([0, 1])
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('RÂ² Score')
        axes[0, 1].legend()
        axes[0, 1].grid(alpha=0.3)
        axes[0, 1].set_title(f'RÂ² Score - Best: {best_val_r2:.4f}')

        # Learning rate
        axes[1, 0].plot(history['lr'], 'g-', linewidth=2)
        axes[1, 0].set_yscale('log')
        axes[1, 0].set_xlabel('Epoch')
        axes[1, 0].set_ylabel('Learning Rate')
        axes[1, 0].grid(alpha=0.3)
        axes[1, 0].set_title('Learning Rate (Cosine Annealing)')

        # Gap Train-Val
        gap = [abs(t - v) for t, v in zip(history['train_r2'], history['val_r2'])]
        axes[1, 1].plot(gap, 'orange', linewidth=2)
        axes[1, 1].axhline(y=0.05, color='green', linestyle='--', alpha=0.5, label='Good gap')
        axes[1, 1].set_xlabel('Epoch')
        axes[1, 1].set_ylabel('|Train RÂ² - Val RÂ²|')
        axes[1, 1].legend()
        axes[1, 1].grid(alpha=0.3)
        axes[1, 1].set_title('Generalization Gap')

        plt.tight_layout()
        plt.savefig(CHECKPOINT_DIR / f'fno_3d_epoch_{epoch}.png', dpi=150)
        plt.show()

        # Checkpoint numÃ©rotÃ©
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'val_loss': val_loss,
            'val_r2': val_r2,
            'history': history
        }, CHECKPOINT_DIR / f'checkpoint_fno_3d_epoch_{epoch}.pt')

        print(f"  ðŸ’¾ Checkpoint epoch {epoch} saved")

    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    gc.collect()

# ============================================================================
# Ã‰VALUATION FINALE SUR TEST SET
# ============================================================================

print(f"\n{'='*70}")
print("Ã‰VALUATION FINALE SUR TEST SET")
print(f"{'='*70}\n")

# Charger le meilleur modÃ¨le
best_ckpt = torch.load(CHECKPOINT_DIR / "best_fno_3d.pt", weights_only=False)
model.load_state_dict(best_ckpt['model_state_dict'])

# Single-step
test_loss, test_r2_single = validate_fno(model, test_loader, criterion, device)
print(f"Test Single-step: Loss={test_loss:.6f}, RÂ²={test_r2_single:.4f}")

# Rollout 10-step
test_r2_rollout_10 = evaluate_rollout_fno(model, test_loader, device, rollout_steps=10)
print(f"Test Rollout 10-step: RÂ²={test_r2_rollout_10:.4f}")

# Sauvegarder rÃ©sultats finaux
final_results = {
    'model': 'FNO_3D',
    'seed': SEED,
    'split': '70/15/15',
    'best_epoch': best_ckpt['epoch'],
    'best_val_r2': float(best_val_r2),
    'test_single_step_r2': float(test_r2_single),
    'test_rollout_10_r2': float(test_r2_rollout_10),
    'n_params': n_params,
    'timestamp': datetime.now().isoformat()
}

with open(CHECKPOINT_DIR / 'final_results.json', 'w') as f:
    json.dump(final_results, f, indent=2)

print(f"\n{'='*70}")
print("RÃ‰SUMÃ‰ FINAL - FNO 3D")
print(f"{'='*70}")
print(f"  Best Epoch: {best_ckpt['epoch']}")
print(f"  Parameters: {n_params:,}")
print(f"  Best Val RÂ²: {best_val_r2:.4f}")
print(f"  Test Single-step RÂ²: {test_r2_single:.4f}")
print(f"  Test Rollout-10 RÂ²: {test_r2_rollout_10:.4f}")
print(f"{'='*70}")
print(f"\nðŸ“Š FNO est excellent pour PDEs stationnaires")
print(f"   mais peut Ãªtre moins stable en rollout pour UHS cyclique")
print(f"{'='*70}\n")